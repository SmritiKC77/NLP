{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "D5oa2xXh1yQs"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Lowercasing"
      ],
      "metadata": {
        "id": "r3AURwEF69dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text =\" Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ],
      "metadata": {
        "id": "rX92gPlE2EcW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Qffw78qX8OE5",
        "outputId": "90ed1bb2-0830-4c30-afb6-1a4d49097b6e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Remove html tags"
      ],
      "metadata": {
        "id": "kNYNY3iS9Kz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text_without_html = re.sub(r'<.*?>', '', text)\n",
        "print(text_without_html)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLd2UypF9ESW",
        "outputId": "428ee404-f474-40ac-e4d6-80bbe9f9ccb3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Remove URL's"
      ],
      "metadata": {
        "id": "nySgx3oM9Heo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = 'Check out my notebook https://www.example.com/notebook8223fc1abb'\n",
        "text2 = 'Check out my notebook http://www.example.com/notebook8223fc1abb'\n",
        "text3 = 'Google search here www.example.com'\n",
        "text4 = 'For notebook click https://www.example.com/notebook8223fc1abb to search check www.example.com'\n",
        "\n"
      ],
      "metadata": {
        "id": "WQ5KXaJC-BtV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_url(text):\n",
        "  pattern = re.compile(r'https?://\\S+|www.\\.\\S+')\n",
        "  return pattern.sub(r'',text)"
      ],
      "metadata": {
        "id": "JBJxyvT_-kIw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_url(text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "15PHn09R_RzW",
        "outputId": "2dcc5c4e-f7c1-4f56-9bd0-2523dabd6c21"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Check out my notebook '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Remove Punctuation"
      ],
      "metadata": {
        "id": "_lKIyTpl2KzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0yzvP3Pb_eyi",
        "outputId": "2f0e6ab0-0a2e-407f-e8dc-e0f725c9b7e1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove = string.punctuation"
      ],
      "metadata": {
        "id": "7QURGu5T_trk"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(text):\n",
        "    for char in remove:\n",
        "        text = text.replace(char,'')\n",
        "    return text"
      ],
      "metadata": {
        "id": "YRnkMTK9_7JY"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= 'what is. your. name?'"
      ],
      "metadata": {
        "id": "-d6mPC0RAB1I"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_punc(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dRt08d2IAKXQ",
        "outputId": "baf349a8-eb05-496a-d18e-5232581cf946"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what is your name'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Chatword Treatment"
      ],
      "metadata": {
        "id": "Kiye0f7dAUSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_conversion(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.lower() in chat_words:\n",
        "            new_text.append(chat_words[w.lower()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)\n"
      ],
      "metadata": {
        "id": "MhCIP04yATu3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_words={\n",
        "  \"lol\": \"laugh out loud\",\n",
        "    \"brb\": \"be right back\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"idk\": \"I don't know\",\n",
        "    \"ttyl\": \"talk to you later\",\n",
        "    \"thx\": \"thanks\",\n",
        "    \"np\": \"no problem\",\n",
        "    \"g2g\": \"got to go\",\n",
        "    \"jk\": \"just kidding\"\n",
        "}"
      ],
      "metadata": {
        "id": "0bAldHThCw6v"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_conversion('LOL it was so funny')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gR7o1SzxANW_",
        "outputId": "7425d6c7-62f9-4cc9-ba5e-0a447a800d6d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'laugh out loud it was so funny'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_conversion('omg there was a snake')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yqo69ZVOCJsW",
        "outputId": "d4e36267-235b-43ea-8da0-2fa9b9dbef38"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'oh my god there was a snake'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Spelling Correction"
      ],
      "metadata": {
        "id": "W9bQtjpLDdnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "cIOxEAdADbYM"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'\n",
        "\n",
        "textBlb = TextBlob(incorrect_text)\n",
        "\n",
        "textBlb.correct().string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LZbIZCvrDrt8",
        "outputId": "fc7b7d21-f024-426a-fcd5-22f89a9d2869"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'certain conditions during several generations are modified in the same manner.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Stopwords Removal"
      ],
      "metadata": {
        "id": "nf-_A8SkDwTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "830QaDyVDuVV",
        "outputId": "e074bb9f-6935-4407-feb2-2ffd13c21f7a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mVVpye-D3LG",
        "outputId": "787a740f-c33d-4a1a-bbb7-1a3152c0180b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    new_text = []\n",
        "\n",
        "    for word in text.split():\n",
        "        if word in stopwords.words('english'):\n",
        "            new_text.append('')\n",
        "        else:\n",
        "            new_text.append(word)\n",
        "    x = new_text[:]\n",
        "    new_text.clear()\n",
        "    return \" \".join(x)"
      ],
      "metadata": {
        "id": "uwex_vioECrX"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times'"
      ],
      "metadata": {
        "id": "7XdeA6toElM2"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_stopwords(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HLun6i_4E71v",
        "outputId": "7c1ac176-bf75-4edd-bb87-d1cf0285c140"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'probably  all-time favorite movie,  story  selflessness, sacrifice  dedication   noble cause,    preachy  boring.   never gets old, despite   seen   15   times'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Tokenizattion\n",
        "\n",
        " Process of breaking text documents into smaller parts i.e words, sentences, phrases called tokens"
      ],
      "metadata": {
        "id": "zdc_dLOZFQR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i. Using Split function"
      ],
      "metadata": {
        "id": "H3ZpOz85FrN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word tokenization\n",
        "sent1 = 'I am from country Nepal'\n",
        "sent1.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgONgRAPE-7w",
        "outputId": "45ca5206-30b6-45e6-bf0a-18948134630a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'from', 'country', 'Nepal']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence tokenization\n",
        "sent2 = 'I am going to America. I will stay there for 3 days. Let\\'s hope the trip to be great'\n",
        "sent2.split('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-IAAemuGai9",
        "outputId": "56409e37-2d71-4139-f44b-64e25e5cf70a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am going to America',\n",
              " ' I will stay there for 3 days',\n",
              " \" Let's hope the trip to be great\"]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problems with split function\n",
        "sent3 = 'I am going to Nepal!'\n",
        "sent3.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEZFe3DbGjjb",
        "outputId": "15ee6d5e-1f98-4029-8f0e-ee5fc45d0eeb"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'going', 'to', 'Nepal!']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It will consider Nepal! as one word"
      ],
      "metadata": {
        "id": "gQMBy3DUGs_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii. Using NLTK"
      ],
      "metadata": {
        "id": "4roIC8s7G7Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQgiu9W4G0Wp",
        "outputId": "06c452c3-7f7a-4695-a939-d5bca5ee0d48"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent3 = 'I am going to Nepal!'\n",
        "word_tokenize(sent3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl18lBqsIfl0",
        "outputId": "80d7d23f-e21a-43db-e24e-0bfdc73a4fc3"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'going', 'to', 'Nepal', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
        "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
        "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
        "\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQthGvgmIrME",
        "outputId": "ea219116-a1fc-43ec-b37f-76ecd26e519d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
              " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent5 = 'I have a Ph.D in A.I'\n",
        "sent6 = \"We're here to help! mail us at kcsmriti@gmail.com\"\n",
        "sent7 = 'A 5km ride cost $10.50'\n",
        "\n",
        "word_tokenize(sent5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDdp2tjxJCpQ",
        "outputId": "259bdff6-79fc-4a4f-f5e1-c3103d20eb33"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(sent6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4IRJ2sNJNCx",
        "outputId": "a35c5e55-b3fa-478a-ea73-246ec71daf06"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We',\n",
              " \"'re\",\n",
              " 'here',\n",
              " 'to',\n",
              " 'help',\n",
              " '!',\n",
              " 'mail',\n",
              " 'us',\n",
              " 'at',\n",
              " 'kcsmriti',\n",
              " '@',\n",
              " 'gmail.com']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this the emailaddress was supposed to be a same word but it is considered differntly,it is a demerit of using nltk. So the best solution for tokenization is using spacy library"
      ],
      "metadata": {
        "id": "n85J60zTJhMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(sent7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yynHK6cJVrX",
        "outputId": "036ebbfe-774c-4904-e66c-f0be25dcd096"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', '5km', 'ride', 'cost', '$', '10.50']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii. Spacy"
      ],
      "metadata": {
        "id": "KrAfWCiDKGIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm') #The 'en' in 'en_core_web_sm' stands for English, indicating that this model is for processing English text. 'core' refers to the fact that it is a core or general-purpose model, and 'web_sm' indicates that it is a small version of the model optimized for speed and efficiency."
      ],
      "metadata": {
        "id": "a9S-y9sCJZoX"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(sent5)\n",
        "doc2 = nlp(sent6)\n",
        "doc3 = nlp(sent7)\n",
        "doc4 = nlp(sent1)"
      ],
      "metadata": {
        "id": "gcp1NiWpLmXq"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc4:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeFUw4jkLm03",
        "outputId": "15e44ac4-bcfe-4df9-89b5-008ccaeb1f63"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "am\n",
            "from\n",
            "country\n",
            "Nepal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc3:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55WE62-ULx3j",
        "outputId": "754ec76f-13e5-4d85-dcc7-50e4aa58d7ef"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "5\n",
            "km\n",
            "ride\n",
            "cost\n",
            "$\n",
            "10.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Stemming\n",
        "\n",
        "Stemming is a process used in natural language processing (NLP) to reduce words to their root or base form, called the \"stem.\" The goal of stemming is to normalize words so that variations of the same word are treated as the same word, regardless of their inflections or endings.\n",
        "\n",
        "For example, consider the words \"running,\" \"runs,\" and \"runner.\" The stem of all these words is \"run.\" Stemming algorithms work by removing suffixes or prefixes from words to extract their stems. While stemming can be a useful technique for certain applications, it's important to note that it can sometimes result in stems that are not actual words, as it operates based on rules and patterns rather than linguistic understanding."
      ],
      "metadata": {
        "id": "obaN73-WMbh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "OA5w6EzlMZDf"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"walk walks walking walked\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "stemmed_words = [porter.stem(word) for word in words]\n",
        "\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JckMj9YhOW-m",
        "outputId": "f382285c-bd39-4930-d9b1-073b7506c8eb"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['walk', 'walk', 'walk', 'walk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "def stem_words(text):\n",
        "    return \" \".join([ps.stem(word) for word in text.split()])"
      ],
      "metadata": {
        "id": "Sawt0HIgO_C3"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie'\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZjBkE3EOdrL",
        "outputId": "98e7e03d-0c2c-49c7-e2d7-5da373697ebb"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stem_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "6ukaT-55OxFd",
        "outputId": "00e179e7-da85-43f9-cc2f-d0a212f4b79f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka perform bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and the mother slow awaken to what happen in the world and under her own roof is believ and startl if i had a dozen thumb theyd all be up for thi movi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this probably became probabl through stemming . But the resulted word is not a actual word which is disadvantage of using stemming."
      ],
      "metadata": {
        "id": "n29X_SgBPER0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Lemmatization\n",
        "\n",
        "lemmatization aims to transform words to their base or dictionary form, called the lemma. Unlike stemming, lemmatization considers the context of the word and uses linguistic rules to produce valid words."
      ],
      "metadata": {
        "id": "sgOzuQyqPgLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform lemmatization\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "# Print the lemmatized words\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AcFcMRXOyv_",
        "outputId": "1ad64e2e-77cc-44ca-9223-aa635cb51cad"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['walk', 'walk', 'walking', 'walked']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie'\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "# Lemmatize each token\n",
        "lemmatized_text = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "# Join the lemmatized tokens back into a single string\n",
        "lemmatized_text = ' '.join(lemmatized_text)\n",
        "\n",
        "print(lemmatized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-ulFaQeSOZg",
        "outputId": "83928188-4054-44b2-d73a-7ecb35aa7f7b"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but it not preachy or boring it just never get old despite my having seen it some 15 or more time in the last 25 year paul lukas performance brings tear to my eye and bette davis in one of her very few truly sympathetic role is a delight the kid are a grandma say more like dressedup midget than child but that only make them more fun to watch and the mother slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumb theyd all be up for this movie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* \"gets\" is lemmatized to \"get\".\n",
        "* \"times\" is lemmatized to \"time\".\n",
        "* \"tears\" is lemmatized to \"tear\".\n",
        "* \"roles\" is lemmatized to \"role\".\n",
        "* \"kids\" is lemmatized to \"kid\".\n",
        "* \"dressedup\" is lemmatized to \"dressedup\".\n",
        "* \"midgets\" is lemmatized to \"midget\".\n",
        "* \"thumbs\" is lemmatized to \"thumb\"."
      ],
      "metadata": {
        "id": "g84NUcnOTiiE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U2Qaw9TcS-FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZYcqS0LETQ4S"
      }
    }
  ]
}